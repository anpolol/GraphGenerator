{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential as Seq\n",
    "\n",
    "from torch_geometric.nn import GCNConv, SAGEConv,GATConv, APPNP, FAConv, SuperGATConv,GINConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Функция для загрузки данных, полученных после использования нашего генератора (например с использованием файла examples/example_generate_graph или examples/example_generate_tuning)\n",
    "def data_load(name):\n",
    "    x =  torch.DoubleTensor(np.load('../dataset/graph_'+str(name)+'_attr.npy'))\n",
    "    edge_list = torch.tensor(np.load('../dataset/graph_'+str(name)+'_edgelist.npy')).t()\n",
    "    y =  torch.tensor(np.load('../dataset/graph_'+str(name)+'_labels.npy'),dtype=torch.long)\n",
    "    data = Data(x=x,edge_index=edge_list,y=y)\n",
    "    train_indices = torch.tensor(np.random.choice(list(np.arange(len(data.x))),int(len(data.x)*0.7), replace = False),dtype=torch.long)\n",
    "    rest_indices = list(set(list(range(len(data.x)))) - set(train_indices.tolist()))\n",
    "    val_indices = torch.tensor(np.random.choice(rest_indices,int(len(data.x)*0.2), replace = False),dtype=torch.long)\n",
    "    test_indices = torch.tensor(list(set(rest_indices) - set(val_indices.tolist())),dtype=torch.long)\n",
    "\n",
    "    #делаем masks\n",
    "    train_mask = torch.tensor([False]*len(data.x))\n",
    "    train_mask[train_indices]=torch.tensor([True]*len(train_indices))\n",
    "    val_mask = torch.tensor([False]*len(data.x))\n",
    "    val_mask[val_indices]=torch.tensor([True]*len(val_indices))\n",
    "    test_mask = torch.tensor([False]*len(data.x))\n",
    "    test_mask[test_indices]=torch.tensor([True]*len(test_indices))\n",
    "    return data, train_indices,val_indices,test_indices,train_mask,val_mask,test_mask\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Класс для формирования модели машинного обучения=графовой нейронной сети\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dataset, device,conv='GCN',hidden_layer=64,out_layer =128,dropout = 0,num_layers=2,**kwargs):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = conv\n",
    "        self.num_layers = num_layers\n",
    "        self.data = dataset\n",
    "        self.num_features = dataset.x.shape[1]\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.out_layer = out_layer\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.alpha = 0.2\n",
    "        if self.conv == 'APPNP':\n",
    "            self.alpha=kwargs[\"alpha\"]\n",
    "\n",
    "\n",
    "        out_channels =len(collections.Counter(self.data.y.tolist()).keys())\n",
    "        if self.conv == 'GCN':\n",
    "            if self.num_layers == 1:\n",
    "                self.convs.append(GCNConv(self.num_features, out_channels))\n",
    "            else:\n",
    "                self.convs.append(GCNConv(self.num_features, self.hidden_layer))\n",
    "                for i in range(1,self.num_layers-1):\n",
    "                    self.convs.append(GCNConv(self.hidden_layer, self.hidden_layer))\n",
    "                self.convs.append(GCNConv(self.hidden_layer, out_channels))\n",
    "        if self.conv == 'GIN':\n",
    "\n",
    "\n",
    "            if self.num_layers == 1:\n",
    "                module = Seq(Linear(self.num_features, self.hidden_layer), nn.ReLU(), Linear(self.hidden_layer, out_channels))\n",
    "                self.convs.append(GINConv(module,train_eps=True))\n",
    "            else:\n",
    "                module = Seq(Linear(self.num_features, self.hidden_layer), nn.ReLU())\n",
    "                self.convs.append(GINConv(module,train_eps=True))\n",
    "                for i in range(1,self.num_layers-1):\n",
    "                    module = Seq(nn.Linear(self.hidden_layer, self.hidden_layer), nn.ReLU())\n",
    "                    self.convs.append(GINConv(module,train_eps=True))\n",
    "                module = Seq(nn.Linear(self.hidden_layer, out_channels))\n",
    "                self.convs.append(GINConv(module,train_eps=True))\n",
    "\n",
    "        elif self.conv=='APPNP':\n",
    "            self.convs.append(Linear(self.num_features, self.hidden_layer))\n",
    "            self.convs.append(Linear(self.hidden_layer, self.out_layer))\n",
    "            self.convs.append(APPNP(self.num_layers,self.alpha, self.dropout))#self.num_features, out_channels))\n",
    "\n",
    "        elif self.conv == 'SAGE':\n",
    "\n",
    "            if self.num_layers == 1:\n",
    "                self.convs.append(SAGEConv(self.num_features, out_channels))\n",
    "            else:\n",
    "                self.convs.append(SAGEConv(self.num_features, self.hidden_layer))\n",
    "                for i in range(1,self.num_layers-1):\n",
    "                    self.convs.append(SAGEConv(self.hidden_layer, self.hidden_layer))\n",
    "                self.convs.append(SAGEConv(self.hidden_layer, out_channels))\n",
    "        elif self.conv == 'GAT':\n",
    "            if self.num_layers == 1:\n",
    "                self.convs.append(GATConv(self.num_features, out_channels))\n",
    "            else:\n",
    "                self.convs.append(GATConv(self.num_features, self.hidden_layer))\n",
    "                for i in range(1,self.num_layers-1):\n",
    "                    self.convs.append(GATConv(self.hidden_layer, self.hidden_layer))\n",
    "                self.convs.append(GATConv(self.hidden_layer, out_channels))\n",
    "\n",
    "        elif self.conv == 'FA':\n",
    "\n",
    "\n",
    "            self.convs.append(Linear(self.num_features, self.hidden_layer))\n",
    "            for i in range(self.num_layers):\n",
    "                    self.convs.append(FAConv(self.hidden_layer, self.hidden_layer))\n",
    "            self.convs.append(Linear(self.hidden_layer, out_channels))\n",
    "\n",
    "        elif self.conv == 'SuperGAT':\n",
    "\n",
    "            if self.num_layers == 1:\n",
    "                self.convs.append(SuperGATConv(self.num_features, out_channels, heads=8,\n",
    "                                  dropout=0.6, attention_type='MX',\n",
    "                                  edge_sample_ratio=0.8, is_undirected=True))\n",
    "            else:\n",
    "                self.convs.append(SuperGATConv(self.num_features, self.hidden_layer, heads=8,\n",
    "                                  dropout=0.6, attention_type='MX',\n",
    "                                  edge_sample_ratio=0.8, is_undirected=True))\n",
    "                for i in range(1,self.num_layers-1):\n",
    "                    self.convs.append(SuperGATConv(self.hidden_layer, self.hidden_layer, heads=8,\n",
    "                                  dropout=0.6, attention_type='MX',\n",
    "                                  edge_sample_ratio=0.8, is_undirected=True))\n",
    "                self.convs.append(SuperGATConv(self.hidden_layer, out_channels, heads=8,\n",
    "                                  dropout=0.6, attention_type='MX',\n",
    "                                  edge_sample_ratio=0.8, is_undirected=True))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self,x,adjs):\n",
    "        if self.conv == 'FA':\n",
    "                x = self.convs[0](x)\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=self.dropout, training=True)\n",
    "                x_o = x\n",
    "                for i, (edge_index, _, size) in enumerate(adjs):\n",
    "                        x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "                        x = self.convs[i+1](x,x_o, edge_index)\n",
    "                        x = F.dropout(x, p=self.dropout, training=True)\n",
    "                x=self.convs[len(self.convs)-1](x)\n",
    "                x = F.dropout(x, p=self.dropout, training=True)\n",
    "        else:\n",
    "            for i, (edge_index, _, size) in enumerate(adjs):\n",
    "                x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "              #  print(x,'\\t',x_target,'\\t',edge_index,'\\t',x.type(),'\\t',x_target.type(),'\\t',edge_index.type())\n",
    "\n",
    "                x = self.convs[i]((x,x_target), edge_index)\n",
    "                if i != self.num_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "                    x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x.log_softmax(dim=1)\n",
    "\n",
    "    def inference(self,data,dp=0,t=True):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if self.conv=='APPNP' and i==0:\n",
    "                x=conv(x)\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=dp, training=t)\n",
    "            elif self.conv=='APPNP' and i==1:\n",
    "                x=conv(x)\n",
    "            elif self.conv=='APPNP' and i>1:\n",
    "                x = conv(x, edge_index)\n",
    "            elif self.conv =='FA' and i == 0:\n",
    "                x=conv(x)\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=dp, training=t)\n",
    "                x_o=x\n",
    "\n",
    "            elif self.conv =='FA' and i == len(self.convs)-1:\n",
    "                x=conv(x)\n",
    "                x = F.dropout(x, p=dp, training=t)\n",
    "            elif self.conv == 'FA' and (i!=0 and i!=len(self.convs)-1):\n",
    "                x = conv(x, x_o, edge_index)\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=dp, training=t)\n",
    "            else:\n",
    "                x = conv(x, edge_index)\n",
    "                if i != self.num_layers - 1:\n",
    "                    x = x.relu()\n",
    "                    x = F.dropout(x, p=dp, training=t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "    def loss_sup(self, pred, label):\n",
    "        return F.nll_loss(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#непосредственно пайплайны тестирования\n",
    "\n",
    "class Main():\n",
    "    def __init__(self,name, conv, device):\n",
    "        data, train_indices,val_indices,test_indices,train_mask,val_mask,test_mask = data_load(name)\n",
    "        self.Conv = conv\n",
    "        self.device = device\n",
    "        self.x = data.x\n",
    "        self.y = data.y.squeeze()\n",
    "        self.data=data.to(device)\n",
    "        \n",
    "        self.train_mask = train_mask#torch.tensor([False]*len(indices))\n",
    "        self.test_mask = test_mask#torch.tensor([False]*len(indices))\n",
    "        self.val_mask =val_mask# torch.tensor([False]*len(indices))\n",
    "        \n",
    "        super(Main, self).__init__()\n",
    "    \n",
    "    def train(self, model,data,optimizer,train_loader,dropout,epoch,alpha):\n",
    "        model.train()   \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        if model.conv=='GCN'  or model.conv =='APPNP' or model.conv == 'FA' or model.conv == 'SuperGAT':\n",
    "                \n",
    "                out = model.inference(data.to(device),dp=dropout,t=True)\n",
    "                y=self.y.to(self.device)\n",
    "                loss = model.loss_sup(out[self.train_mask],y[self.train_mask])\n",
    "                \n",
    "                total_loss+=loss\n",
    "        else:\n",
    "            for batch_size, n_id, adjs in train_loader:\n",
    "                    if len(train_loader.sizes) == 1:\n",
    "                        adjs = [adjs]\n",
    "                    adjs = [adj.to(device) for adj in adjs]\n",
    "                    \n",
    "                \n",
    "                    out = model.forward(data.x[n_id].to(device), adjs)\n",
    "                    y=self.y.to(self.device)\n",
    "                    loss = model.loss_sup(out,y[n_id[:batch_size]])\n",
    "                    total_loss+=loss\n",
    "        total_loss.backward(retain_graph=True)\n",
    "        optimizer.step()      \n",
    "        return total_loss /len(train_loader)       \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, model,data,**kwargs):#,n_estimators,learning_rate_carboost, max_depth): \n",
    "        model.eval()\n",
    "        out = model.inference(data.to(device),t=False)\n",
    "        y_true = self.y.cpu().detach().numpy()\n",
    "        self.y=self.y.cpu()\n",
    "        y_true = self.y.unsqueeze(-1)\n",
    "        y_pred = out.cpu().argmax(dim=-1, keepdim=True)\n",
    "        accs_micro = []\n",
    "        for mask in [self.train_mask,self.test_mask,self.val_mask]:\n",
    "                accs_micro += [f1_score(self.y.detach()[mask].cpu().numpy(),y_pred[mask], average='weighted')]\n",
    "                \n",
    "                \n",
    "        return accs_micro\n",
    "\n",
    "    def run(self,hidden_layer=64,out_layer=128,dropout=0.0,size=1,learning_rate=0.001,alpha=0):\n",
    "        \n",
    "        train_loader = NeighborSampler(self.data.edge_index, node_idx=self.train_mask, batch_size = int(sum(self.train_mask)), sizes=[-1]*size)\n",
    "        model = Net(dataset = self.data,conv = self.Conv,device = device,hidden_layer = hidden_layer,out_layer = out_layer,num_layers = (size),dropout = dropout,alpha=alpha)\n",
    "        model.to(device)\n",
    "        model.double()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)\n",
    "        #scheduler=lr_scheduler.StepLR(optimizer, step_size=25,gamma=0.1)\n",
    "        losses=[]\n",
    "        train_accs=[]\n",
    "        test_accs=[]\n",
    "        val_accs=[]\n",
    "        name_of_plot='conv: '+model.conv\n",
    "        print(name_of_plot)\n",
    "        log = 'Loss: {:.4f}, Epoch: {:03d}, Train acc: {:.4f}, Test acc: {:.4f}'\n",
    "         \n",
    "        for epoch in range(100):\n",
    "                    loss = self.train(model,self.data,optimizer,train_loader,dropout,epoch,alpha)\n",
    "                    losses.append(loss.detach().cpu())\n",
    "                    [train_acc, test_acc,val_acc]= self.test(model,self.data)\n",
    "                    train_accs.append(train_acc)\n",
    "                    test_accs.append(test_acc)\n",
    "                    print(log.format(loss, epoch, train_acc, test_acc ))\n",
    "                     #scheduler.step()\n",
    "        print(log.format(loss, epoch, train_acc, test_acc))\n",
    "        #print('Test acc on the last epoch micro ', test_acc_micro)\n",
    "        #print(list(map(lambda x: float(x),losses)))\n",
    "        plt.plot(losses)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "        plt.plot(test_accs)\n",
    "        plt.title(name_of_plot+' test f1 micro')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainOptuna(Main):\n",
    "    def objective(self,trial):\n",
    "        # Integer parameter\n",
    "        hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "        out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "        size = trial.suggest_categorical(\"size of network, number of convs\", [1,2,3])\n",
    "        Conv = self.Conv\n",
    "        learning_rate= trial.suggest_float(\"lr\",5e-5,1e-2)\n",
    "        alpha=0\n",
    "   \n",
    "        if Conv =='APPNP':\n",
    "            alpha= trial.suggest_float(\"alpha\", 0.1,1,step = 0.1)\n",
    "        if Conv=='FA':\n",
    "            eps = trial.suggest_float(\"eps\", 0.1,1,step = 0.1)\n",
    "            model = Net(dataset = self.data,eps=eps,conv=Conv,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = size,dropout = dropout,alpha=alpha)\n",
    "        else:\n",
    "            model = Net(dataset = self.data,conv=Conv,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = size,dropout = dropout,alpha=alpha)\n",
    "        model.double()\n",
    "        train_loader = NeighborSampler(self.data.edge_index, batch_size = int(sum(self.train_mask)),node_idx=self.train_mask, sizes=[-1]*size)\n",
    "       # train_loader = NeighborSampler(self.data.edge_index, batch_size = int(sum(self.train_mask)), sizes=[-1]*size)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "        \n",
    "            \n",
    "        for epoch in range(100):\n",
    "            #print('hi',self.data.x.shape)\n",
    "            loss = self.train(model,self.data,optimizer,train_loader,dropout,epoch,alpha)\n",
    "            #self.test(model,self.data,classifier,n_estimators=n_estimators,learning_rate_catboost=learning_rate_catboost,max_depth=max_depth)\n",
    "        [train_acc, test_acc,val_acc]= self.test(model,self.data)\n",
    "        trial.report( val_acc ,epoch)\n",
    "        return np.sqrt(val_acc)\n",
    "\n",
    "    \n",
    "    def run(self,number_of_trials):\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\",study_name=str(self.Conv)+\" conv\")\n",
    "        study.optimize(self.objective,n_trials = number_of_trials)\n",
    "\n",
    "        print('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        print(\" Value: \", trial.value)\n",
    "        print(\" Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(\" {}: {}\".format(key,value))\n",
    "        return study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names=[]\n",
    "a=0\n",
    "for l_a_trgt in [0.1,0.2,0.3,0.4,0.6,0.7,0.8,0.5,0.9]:\n",
    "    for f_a_trgt in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.75,0.8,0.85,0.9,0.95]:\n",
    "        for cl_trgt in [0.01,0.1,0.2,0.25,0.5]:\n",
    "            for asp_trgt in [2,3,4,4.5,5,5.5,6,6.5,7]:\n",
    "                for a_deg_trgt in [2,5,20,40]:\n",
    "\n",
    "                    name =  \"\".join(list(map(lambda x:str(x),  [l_a_trgt,f_a_trgt,cl_trgt,asp_trgt,a_deg_trgt])))\n",
    "                   # print(name)\n",
    "                    if os.path.exists('../dataset/graph_'+str(name)+'_labels.npy'):\n",
    "                        datasets_names.append((l_a_trgt,f_a_trgt,cl_trgt,asp_trgt,a_deg_trgt))\n",
    "                        a+=1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Тестируем метод APPNP на кусочке нашего датасета\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['label assort','feature assort','cluster','average shortest paths','average degree','conv','test accuracy'])\n",
    "for (l,f,cl,asp,ad) in datasets_names:\n",
    "    name =  \"\".join(list(map(lambda x:str(x),  [l,f,cl,asp,ad])))\n",
    "    #print(name)\n",
    "    if os.path.exists('../dataset/graph_'+str(name)+'_labels.npy'):\n",
    "        for conv in ['APPNP']:\n",
    "                #print(conv)\n",
    "                if len(df_results[(df_results['conv'] == conv) & (df_results['label assort'] == l) & (df_results['feature assort'] == f) &(df_results['cluster'] == cl) & (df_results['average shortest paths'] == asp) & (df_results['average degree'] == ad)] ) == 0:\n",
    "                    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    print(device)\n",
    "                    MO = MainOptuna(name,conv, device)\n",
    "\n",
    "                    best_trial = MO.run(number_of_trials=250)\n",
    "                    Model = Main(name,conv, device)\n",
    "\n",
    "                    test_acc = Model.run(hidden_layer=best_trial['hidden_layer'],out_layer=best_trial['out_layer'],dropout=best_trial['dropout'],size=best_trial['size of network, number of convs'],learning_rate=best_trial['lr'])\n",
    "                    print(test_acc)\n",
    "                    to_append = [l,f,cl,asp,ad,conv,test_acc]\n",
    "                    print(to_append)\n",
    "                    row_series = pd.Series(to_append, index = df_results.columns)\n",
    "                    df_results = df_results.append(row_series, ignore_index = True)\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
